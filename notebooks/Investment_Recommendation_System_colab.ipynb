{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investment Recommendation System\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive investment recommendation system that analyzes both stocks and mutual funds to provide personalized investment recommendations. The system uses multiple machine learning models for price prediction, risk analysis, and portfolio optimization.\n",
    "\n",
    "## Features\n",
    "1. Data Processing and Analysis\n",
    "   - Stock data analysis (12 stocks)\n",
    "   - Mutual fund data analysis (6 funds)\n",
    "   - Technical indicators\n",
    "   - Feature engineering\n",
    "\n",
    "2. Machine Learning Models\n",
    "   - LSTM for short-term predictions\n",
    "   - ARIMA-GARCH for medium-term predictions\n",
    "   - Prophet for long-term forecasting\n",
    "\n",
    "3. Risk Analysis\n",
    "   - Volatility calculation\n",
    "   - Sharpe ratio\n",
    "   - Risk classification\n",
    "   - Portfolio risk assessment\n",
    "\n",
    "4. Portfolio Optimization\n",
    "   - Modern Portfolio Theory\n",
    "   - Risk-adjusted allocation\n",
    "   - Balanced portfolio strategy\n",
    "\n",
    "5. Results and Reporting\n",
    "   - Price predictions\n",
    "   - Risk profiles\n",
    "   - Portfolio recommendations\n",
    "   - Visual reports\n",
    "\n",
    "   ## Prerequisites\n",
    "   - Python 3.8+\n",
    "   - Google Colab with GPU runtime\n",
    "   - Required files:\n",
    "     - 12 stock files (stock_1.csv/xlsx to stock_12.csv/xlsx)\n",
    "     - 6 mutual fund files (mf_1.csv/xlsx to mf_6.csv/xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.0 GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the GPU check and installation block with:\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Install packages with compatible versions\n",
    "%pip install numpy pandas scikit-learn tensorflow keras yfinance ta prophet pmdarima arch matplotlib seaborn plotly openpyxl\n",
    "\n",
    "# Add GPU memory management\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from prophet import Prophet\n",
    "from arch import arch_model\n",
    "import ta\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Local Data Section\n",
    "import os\n",
    "from google.colab import files  # For Google Colab\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "def upload_and_process_files():\n",
    "    \"\"\"Upload files and categorize them based on their content\"\"\"\n",
    "    print(\"Please upload your stock and mutual fund files (CSV or Excel)...\")\n",
    "    uploaded = files.upload()  # This opens a file picker dialog\n",
    "    \n",
    "    stock_data = {}\n",
    "    mf_data = {}\n",
    "    \n",
    "    for filename in uploaded.keys():\n",
    "        # Save the uploaded file\n",
    "        file_path = f'data/raw/{filename}'\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(uploaded[filename])\n",
    "        \n",
    "        try:\n",
    "            # Check file extension to determine how to read it\n",
    "            if filename.lower().endswith('.xlsx') or filename.lower().endswith('.xls'):\n",
    "                df = pd.read_excel(file_path)\n",
    "                print(f\"Reading as Excel file: {filename}\")\n",
    "            else:\n",
    "                df = pd.read_csv(file_path)\n",
    "                print(f\"Reading as CSV file: {filename}\")\n",
    "            \n",
    "            # Convert Date column to datetime if present\n",
    "            if 'Date' in df.columns:\n",
    "                df['Date'] = pd.to_datetime(df['Date'])\n",
    "            \n",
    "            # For mutual funds, look for NAV column\n",
    "            if 'NAV' in df.columns:\n",
    "                mf_data[filename] = df\n",
    "                print(f\"‚úÖ Processed as MUTUAL FUND data: {filename}\")\n",
    "                continue\n",
    "                \n",
    "            # For stocks, check for typical stock columns\n",
    "            stock_cols = ['Open', 'High', 'Low', 'Close']\n",
    "            if any(col in df.columns for col in stock_cols):\n",
    "                stock_data[filename] = df\n",
    "                print(f\"‚úÖ Processed as STOCK data: {filename}\")\n",
    "                continue\n",
    "                \n",
    "            # If we reach here, we couldn't categorize the file\n",
    "            print(f\"‚ùì Couldn't automatically categorize {filename}\")\n",
    "            print(f\"\\nColumns in {filename}: {df.columns.tolist()}\")\n",
    "            \n",
    "            # In Colab we can't use input(), so we'll make a best guess\n",
    "            if 'Volume' in df.columns:\n",
    "                stock_data[filename] = df\n",
    "                print(f\"‚úÖ Guessed as STOCK data (has Volume column): {filename}\")\n",
    "            else:\n",
    "                # Default to MF if we can't determine\n",
    "                mf_data[filename] = df\n",
    "                print(f\"‚úÖ Guessed as MUTUAL FUND data: {filename}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {str(e)}\")\n",
    "            print(f\"This might be due to file format issues. Check if {filename} is properly formatted.\")\n",
    "    \n",
    "    print(f\"\\nSummary: Loaded {len(stock_data)} stock files and {len(mf_data)} mutual fund files\")\n",
    "    return stock_data, mf_data\n",
    "\n",
    "# Run the upload and categorization\n",
    "stock_data, mf_data = upload_and_process_files()\n",
    "\n",
    "# Display sample of loaded data\n",
    "if stock_data:\n",
    "    print(\"\\nüìä Sample of Stock Data:\")\n",
    "    sample_stock = list(stock_data.values())[0]\n",
    "    file_name = list(stock_data.keys())[0]\n",
    "    print(f\"From file: {file_name}\")\n",
    "    display(sample_stock.head())\n",
    "    print(f\"Columns: {sample_stock.columns.tolist()}\")\n",
    "\n",
    "if mf_data:\n",
    "    print(\"\\nüìä Sample of Mutual Fund Data:\")\n",
    "    sample_mf = list(mf_data.values())[0]\n",
    "    file_name = list(mf_data.keys())[0]\n",
    "    print(f\"From file: {file_name}\")\n",
    "    display(sample_mf.head())\n",
    "    print(f\"Columns: {sample_mf.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2: Data Processing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def prepare_lstm_data(self, df, sequence_length=60):\n",
    "        # Add critical validation\n",
    "        if len(df) < sequence_length:\n",
    "            raise ValueError(f\"Data length ({len(df)}) is less than sequence length ({sequence_length})\")\n",
    "        \n",
    "        if df.isnull().any().any():\n",
    "            raise ValueError(\"Data contains NaN values. Please clean the data first.\")\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.stock_scaler = MinMaxScaler()\n",
    "        self.mf_scaler = MinMaxScaler()\n",
    "        \n",
    "    def load_stock_data(self, file_path):\n",
    "        \"\"\"Load stock data from CSV file.\"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.sort_values('Date')\n",
    "        return df\n",
    "    \n",
    "    def load_mutual_fund_data(self, file_path):\n",
    "        \"\"\"Load mutual fund data from CSV file.\"\"\"\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.sort_values('Date')\n",
    "        return df\n",
    "    \n",
    "    def add_technical_indicators(self, df):\n",
    "        \"\"\"Add technical indicators to the dataset.\"\"\"\n",
    "        # RSI\n",
    "        df['RSI'] = ta.momentum.RSIIndicator(df['price']).rsi()\n",
    "        \n",
    "        # MACD\n",
    "        macd = ta.trend.MACD(df['price'])\n",
    "        df['MACD'] = macd.macd()\n",
    "        df['MACD_Signal'] = macd.macd_signal()\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        bollinger = ta.volatility.BollingerBands(df['price'])\n",
    "        df['BB_High'] = bollinger.bollinger_hband()\n",
    "        df['BB_Low'] = bollinger.bollinger_lband()\n",
    "        \n",
    "        # Volume indicators\n",
    "        df['Volume_MA'] = ta.trend.SMAIndicator(df['Volume'], window=20).sma_indicator()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_lstm_data(self, df, sequence_length=60):\n",
    "        \"\"\"Prepare data for LSTM model training.\"\"\"\n",
    "        # Select features for LSTM\n",
    "        features = ['open', 'High', 'Low', 'price', 'Volume', 'RSI', 'MACD', 'MACD_Signal']\n",
    "        data = df[features].values\n",
    "        \n",
    "        # Scale the data\n",
    "        scaled_data = self.stock_scaler.fit_transform(data)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(len(scaled_data) - sequence_length):\n",
    "            X.append(scaled_data[i:(i + sequence_length)])\n",
    "            y.append(scaled_data[i + sequence_length, 3])  # Predict price\n",
    "            \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def prepare_mf_lstm_data(self, df, sequence_length=60):\n",
    "        \"\"\"Prepare mutual fund data for LSTM model training.\"\"\"\n",
    "        # Select features for LSTM\n",
    "        features = ['NAV']\n",
    "        data = df[features].values\n",
    "        \n",
    "        # Scale the data\n",
    "        scaled_data = self.mf_scaler.fit_transform(data)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(len(scaled_data) - sequence_length):\n",
    "            X.append(scaled_data[i:(i + sequence_length)])\n",
    "            y.append(scaled_data[i + sequence_length, 0])  # Predict NAV\n",
    "            \n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def prepare_prophet_data(self, df, is_mf=False):\n",
    "        \"\"\"Prepare data for Prophet model.\"\"\"\n",
    "        if is_mf:\n",
    "            prophet_data = df[['Date', 'NAV']].copy()\n",
    "            prophet_data.columns = ['ds', 'y']\n",
    "        else:\n",
    "            prophet_data = df[['Date', 'price']].copy()\n",
    "            prophet_data.columns = ['ds', 'y']\n",
    "        return prophet_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_column_names(df):\n",
    "    \"\"\"Standardize column names across different file formats.\"\"\"\n",
    "    column_mapping = {\n",
    "        'Close': 'Price',\n",
    "        'Adj Close': 'Price',\n",
    "        'Net Asset Value': 'NAV',\n",
    "        'Net Asset Value (NAV)': 'NAV'\n",
    "    }\n",
    "    df.columns = [column_mapping.get(col, col) for col in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data cleaning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(self, df):\n",
    "   \"\"\"Clean and preprocess data.\"\"\"\n",
    "   # Remove duplicates\n",
    "   df = df.drop_duplicates()\n",
    "   # Forward fill missing values\n",
    "   df = df.fillna(method='ffill')\n",
    "   # Backward fill any remaining missing values\n",
    "   df = df.fillna(method='bfill')\n",
    "   return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3: Model Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def train_lstm(self, X_train, y_train, X_val, y_val):\n",
    "        # Add critical validation\n",
    "        if X_train.shape[0] == 0 or y_train.shape[0] == 0:\n",
    "            raise ValueError(\"Empty training data\")\n",
    "            \n",
    "        if X_train.shape[1] != X_val.shape[1]:\n",
    "            raise ValueError(\"Training and validation data have different feature dimensions\")\n",
    "            \n",
    "        # Add early stopping to prevent overfitting\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    def __init__(self):\n",
    "        self.stock_models = {}\n",
    "        self.mf_models = {}\n",
    "        \n",
    "    def create_lstm_model(self, input_shape):\n",
    "        \"\"\"Create and compile LSTM model.\"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(50, return_sequences=True, input_shape=input_shape),\n",
    "            Dropout(0.2),\n",
    "            LSTM(50, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def train_lstm(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train LSTM model.\"\"\"\n",
    "        model = self.create_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return model, history\n",
    "    \n",
    "    def train_prophet(self, data):\n",
    "        \"\"\"Train Prophet model.\"\"\"\n",
    "        model = Prophet(daily_seasonality=True)\n",
    "        model.fit(data)\n",
    "        return model\n",
    "    \n",
    "    def train_arima_garch(self, data):\n",
    "        \"\"\"Train ARIMA-GARCH model.\"\"\"\n",
    "        # Fit ARIMA model\n",
    "        arima = arch.arch_model(data, vol='Garch', p=1, q=1)\n",
    "        results = arima.fit(disp='off')\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 4: Risk Analysis Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskAnalyzer:\n",
    "    def analyze_stock_risk(self, data):\n",
    "        # Add critical validation\n",
    "        if data.empty:\n",
    "            raise ValueError(\"Empty data provided for risk analysis\")\n",
    "            \n",
    "        if 'price' not in data.columns:\n",
    "            raise ValueError(\"Price column not found in data\")\n",
    "            \n",
    "    def __init__(self):\n",
    "        self.stock_risk_metrics = {}\n",
    "        self.mf_risk_metrics = {}\n",
    "        \n",
    "    def calculate_volatility(self, returns, window=252):\n",
    "        \"\"\"Calculate annualized volatility.\"\"\"\n",
    "        return returns.std() * np.sqrt(252)\n",
    "    \n",
    "    def calculate_sharpe_ratio(self, returns, risk_free_rate=0.02):\n",
    "        \"\"\"Calculate Sharpe ratio.\"\"\"\n",
    "        excess_returns = returns - risk_free_rate/252\n",
    "        return np.sqrt(252) * excess_returns.mean() / excess_returns.std()\n",
    "    \n",
    "    def calculate_max_drawdown(self, prices):\n",
    "        \"\"\"Calculate maximum drawdown.\"\"\"\n",
    "        peak = prices.expanding(min_periods=1).max()\n",
    "        drawdown = (prices - peak) / peak\n",
    "        return drawdown.min()\n",
    "    \n",
    "    def classify_risk(self, volatility):\n",
    "        \"\"\"Classify risk based on volatility.\"\"\"\n",
    "        if volatility < 0.15:\n",
    "            return 'Low'\n",
    "        elif volatility < 0.30:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'High'\n",
    "    \n",
    "    def analyze_stock_risk(self, data):\n",
    "        \"\"\"Analyze risk for stock data.\"\"\"\n",
    "        returns = data['price'].pct_change().dropna()\n",
    "        \n",
    "        self.stock_risk_metrics['volatility'] = self.calculate_volatility(returns)\n",
    "        self.stock_risk_metrics['sharpe_ratio'] = self.calculate_sharpe_ratio(returns)\n",
    "        self.stock_risk_metrics['max_drawdown'] = self.calculate_max_drawdown(data['price'])\n",
    "        self.stock_risk_metrics['risk_category'] = self.classify_risk(self.stock_risk_metrics['volatility'])\n",
    "        \n",
    "        return self.stock_risk_metrics\n",
    "    \n",
    "    def analyze_mf_risk(self, data):\n",
    "        \"\"\"Analyze risk for mutual fund data.\"\"\"\n",
    "        returns = data['NAV'].pct_change().dropna()\n",
    "        \n",
    "        self.mf_risk_metrics['volatility'] = self.calculate_volatility(returns)\n",
    "        self.mf_risk_metrics['sharpe_ratio'] = self.calculate_sharpe_ratio(returns)\n",
    "        self.mf_risk_metrics['max_drawdown'] = self.calculate_max_drawdown(data['NAV'])\n",
    "        self.mf_risk_metrics['risk_category'] = self.classify_risk(self.mf_risk_metrics['volatility'])\n",
    "        \n",
    "        return self.mf_risk_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 5: Portfolio Optimization Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioOptimizer:\n",
    "    def optimize_portfolio(self, returns, num_portfolios=10000):\n",
    "        # Add critical validation\n",
    "        if returns.empty:\n",
    "            raise ValueError(\"No returns data provided for portfolio optimization\")\n",
    "            \n",
    "        if len(returns) < 252:  # Minimum for annual calculations\n",
    "            raise ValueError(\"Insufficient data for portfolio optimization. Need at least 252 days of data.\")\n",
    "            \n",
    "    def __init__(self):\n",
    "        self.stock_weights = None\n",
    "        self.mf_weights = None\n",
    "        self.returns = None\n",
    "        self.risk = None\n",
    "        \n",
    "    def calculate_portfolio_metrics(self, returns, weights):\n",
    "        \"\"\"Calculate portfolio returns and risk.\"\"\"\n",
    "        portfolio_returns = np.sum(returns.mean() * weights) * 252\n",
    "        portfolio_risk = np.sqrt(np.dot(weights.T, np.dot(returns.cov() * 252, weights)))\n",
    "        return portfolio_returns, portfolio_risk\n",
    "    \n",
    "    def optimize_portfolio(self, returns, num_portfolios=10000):\n",
    "        \"\"\"Optimize portfolio weights using Monte Carlo simulation.\"\"\"\n",
    "        num_assets = len(returns.columns)\n",
    "        results = np.zeros((3, num_portfolios))\n",
    "        weights_list = []\n",
    "        \n",
    "        for i in range(num_portfolios):\n",
    "            weights = np.random.random(num_assets)\n",
    "            weights /= np.sum(weights)\n",
    "            weights_list.append(weights)\n",
    "            \n",
    "            portfolio_returns, portfolio_risk = self.calculate_portfolio_metrics(returns, weights)\n",
    "            results[0, i] = portfolio_returns\n",
    "            results[1, i] = portfolio_risk\n",
    "            results[2, i] = portfolio_returns / portfolio_risk  # Sharpe ratio\n",
    "            \n",
    "        # Find optimal portfolio\n",
    "        optimal_idx = np.argmax(results[2])\n",
    "        self.weights = weights_list[optimal_idx]\n",
    "        self.returns = results[0, optimal_idx]\n",
    "        self.risk = results[1, optimal_idx]\n",
    "        \n",
    "        return self.weights, self.returns, self.risk\n",
    "    \n",
    "    def optimize_balanced_portfolio(self, stock_returns, mf_returns, stock_weight=0.6):\n",
    "        \"\"\"Optimize a balanced portfolio with both stocks and mutual funds.\"\"\"\n",
    "        # Optimize stock portfolio\n",
    "        stock_weights, stock_returns, stock_risk = self.optimize_portfolio(stock_returns)\n",
    "        \n",
    "        # Optimize mutual fund portfolio\n",
    "        mf_weights, mf_returns, mf_risk = self.optimize_portfolio(mf_returns)\n",
    "        \n",
    "        # Combine portfolios\n",
    "        combined_returns = stock_weight * stock_returns + (1 - stock_weight) * mf_returns\n",
    "        combined_risk = np.sqrt(stock_weight**2 * stock_risk**2 + (1-stock_weight)**2 * mf_risk**2)\n",
    "        \n",
    "        return {\n",
    "            'stock_weights': stock_weights,\n",
    "            'mf_weights': mf_weights,\n",
    "            'stock_weight': stock_weight,\n",
    "            'combined_returns': combined_returns,\n",
    "            'combined_risk': combined_risk\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 6: Main Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add error handling for model training\n",
    "try:\n",
    "    # Initialize components\n",
    "    data_processor = DataProcessor()\n",
    "    model_trainer = ModelTrainer()\n",
    "    risk_analyzer = RiskAnalyzer()\n",
    "    portfolio_optimizer = PortfolioOptimizer()\n",
    "    \n",
    "    # Process data\n",
    "    for file, data in stock_data.items():\n",
    "        try:\n",
    "            # Prepare data\n",
    "            X, y = data_processor.prepare_lstm_data(data)\n",
    "            \n",
    "            # Train model\n",
    "            model, history = model_trainer.train_lstm(X_train, y_train, X_val, y_val)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Fatal error in main execution: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 7: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess stock data\n",
    "def upload_and_process_files():\n",
    "    # Add critical validation\n",
    "    if not uploaded:\n",
    "        raise ValueError(\"No files were uploaded. Please upload your data files.\")\n",
    "        \n",
    "    for filename, df in stock_data.items():\n",
    "        if len(df) < 60:  # Minimum required for LSTM sequence\n",
    "            raise ValueError(f\"Insufficient data points in {filename}. Need at least 60 data points.\")\n",
    "            \n",
    "print(\"Loading stock data...\")\n",
    "stock_data = {}\n",
    "for file in stock_files:\n",
    "    try:\n",
    "        data = data_processor.load_stock_data(file)\n",
    "        data = data_processor.add_technical_indicators(data)\n",
    "        stock_data[file] = data\n",
    "        print(f\"Loaded {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {str(e)}\")\n",
    "\n",
    "# Load and preprocess mutual fund data\n",
    "print(\"\\nLoading mutual fund data...\")\n",
    "mf_data = {}\n",
    "for file in mf_files:\n",
    "    try:\n",
    "        data = data_processor.load_mutual_fund_data(file)\n",
    "        mf_data[file] = data\n",
    "        print(f\"Loaded {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print data summary\n",
    "print(\"\\nData Summary:\")\n",
    "print(\"\\nStock Data:\")\n",
    "for file, data in stock_data.items():\n",
    "    print(f\"\\n{file}:\")\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(f\"Date Range: {data['Date'].min()} to {data['Date'].max()}\")\n",
    "    print(f\"Missing Values: {data.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nMutual Fund Data:\")\n",
    "for file, data in mf_data.items():\n",
    "    print(f\"\\n{file}:\")\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(f\"Date Range: {data['Date'].min()} to {data['Date'].max()}\")\n",
    "    print(f\"Missing Values: {data.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 8: Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for stocks\n",
    "print(\"\\nTraining stock models...\")\n",
    "for file, data in stock_data.items():\n",
    "    print(f\"\\nTraining models for {file}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X, y = data_processor.prepare_lstm_data(data)\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_val = X[:train_size], X[train_size:]\n",
    "    y_train, y_val = y[:train_size], y[train_size:]\n",
    "    \n",
    "    # Train LSTM model\n",
    "    model, history = model_trainer.train_lstm(X_train, y_train, X_val, y_val)\n",
    "    model_trainer.stock_models[file] = model\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Model Loss for {file}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 9: Risk Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform risk analysis\n",
    "print(\"\\nPerforming risk analysis...\")\n",
    "stock_risk_profiles = {}\n",
    "mf_risk_profiles = {}\n",
    "\n",
    "for file, data in stock_data.items():\n",
    "    risk_metrics = risk_analyzer.analyze_stock_risk(data)\n",
    "    stock_risk_profiles[file] = risk_metrics\n",
    "    print(f\"\\nRisk Profile for {file}:\")\n",
    "    for metric, value in risk_metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "\n",
    "for file, data in mf_data.items():\n",
    "    risk_metrics = risk_analyzer.analyze_mf_risk(data)\n",
    "    mf_risk_profiles[file] = risk_metrics\n",
    "    print(f\"\\nRisk Profile for {file}:\")\n",
    "    for metric, value in risk_metrics.items():\n",
    "        print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 10: Portfolio Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize portfolio\n",
    "print(\"\\nOptimizing portfolio...\")\n",
    "\n",
    "# Calculate returns for portfolio optimization\n",
    "stock_returns = pd.DataFrame()\n",
    "for file, data in stock_data.items():\n",
    "    stock_returns[file] = data['price'].pct_change()\n",
    "\n",
    "mf_returns = pd.DataFrame()\n",
    "for file, data in mf_data.items():\n",
    "    mf_returns[file] = data['NAV'].pct_change()\n",
    "\n",
    "stock_returns = stock_returns.dropna()\n",
    "mf_returns = mf_returns.dropna()\n",
    "\n",
    "# Optimize balanced portfolio\n",
    "portfolio_results = portfolio_optimizer.optimize_balanced_portfolio(stock_returns, mf_returns)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nOptimal Portfolio Allocation:\")\n",
    "print(\"\\nStocks:\")\n",
    "for file, weight in zip(stock_files, portfolio_results['stock_weights']):\n",
    "    print(f\"{file}: {weight:.2%}\")\n",
    "\n",
    "print(\"\\nMutual Funds:\")\n",
    "for file, weight in zip(mf_files, portfolio_results['mf_weights']):\n",
    "    print(f\"{file}: {weight:.2%}\")\n",
    "\n",
    "print(f\"\\nStock Allocation: {portfolio_results['stock_weight']:.2%}\")\n",
    "print(f\"Mutual Fund Allocation: {(1-portfolio_results['stock_weight']):.2%}\")\n",
    "print(f\"\\nExpected Portfolio Return: {portfolio_results['combined_returns']:.2%}\")\n",
    "print(f\"Portfolio Risk: {portfolio_results['combined_risk']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 11: Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize portfolio allocation\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Stock allocation\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(portfolio_results['stock_weights'], labels=stock_files, autopct='%1.1f%%')\n",
    "plt.title('Stock Portfolio Allocation')\n",
    "\n",
    "# Mutual fund allocation\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(portfolio_results['mf_weights'], labels=mf_files, autopct='%1.1f%%')\n",
    "plt.title('Mutual Fund Portfolio Allocation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 12: Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "\n",
    "stock_predictions = {}\n",
    "mf_predictions = {}\n",
    "\n",
    "# Stock predictions\n",
    "for file, data in stock_data.items():\n",
    "    # Prepare data for prediction\n",
    "    X, _ = data_processor.prepare_lstm_data(data)\n",
    "    last_sequence = X[-1:]\n",
    "    \n",
    "    # Generate predictions\n",
    "    model = model_trainer.stock_models[file]\n",
    "    predicted_scaled = model.predict(last_sequence)\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    predicted_price = data_processor.stock_scaler.inverse_transform(\n",
    "        np.hstack([np.zeros((1, X.shape[2]-1)), predicted_scaled])\n",
    "    )[:, 3]\n",
    "    \n",
    "    stock_predictions[file] = predicted_price[0]\n",
    "    print(f\"Predicted price for {file}: ‚Çπ{predicted_price[0]:.2f}\")\n",
    "\n",
    "# Mutual fund predictions\n",
    "for file, data in mf_data.items():\n",
    "    # Prepare data for prediction\n",
    "    X, _ = data_processor.prepare_mf_lstm_data(data)\n",
    "    last_sequence = X[-1:]\n",
    "    \n",
    "    # Generate predictions\n",
    "    model = model_trainer.mf_models[file]\n",
    "    predicted_scaled = model.predict(last_sequence)\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    predicted_nav = data_processor.mf_scaler.inverse_transform(predicted_scaled)\n",
    "    \n",
    "    mf_predictions[file] = predicted_nav[0][0]\n",
    "    print(f\"Predicted NAV for {file}: ‚Çπ{predicted_nav[0][0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory if it doesn't exist\n",
    "!mkdir -p results\n",
    "\n",
    "# Save stock portfolio allocation\n",
    "stock_allocation = pd.DataFrame({\n",
    "    'Ticker': stock_files,\n",
    "    'Weight': portfolio_results['stock_weights'],\n",
    "    'Risk_Category': [stock_risk_profiles[file]['risk_category'] for file in stock_files],\n",
    "    'Predicted_Price': [stock_predictions[file] for file in stock_files]\n",
    "})\n",
    "stock_allocation.to_csv('results/stock_portfolio_allocation.csv', index=False)\n",
    "\n",
    "# Save mutual fund portfolio allocation\n",
    "mf_allocation = pd.DataFrame({\n",
    "    'Fund': mf_files,\n",
    "    'Weight': portfolio_results['mf_weights'],\n",
    "    'Risk_Category': [mf_risk_profiles[file]['risk_category'] for file in mf_files],\n",
    "    'Predicted_NAV': [mf_predictions[file] for file in mf_files]\n",
    "})\n",
    "mf_allocation.to_csv('results/mf_portfolio_allocation.csv', index=False)\n",
    "\n",
    "# Save portfolio summary\n",
    "portfolio_summary = pd.DataFrame({\n",
    "    'Metric': ['Stock_Allocation', 'Mutual_Fund_Allocation', 'Expected_Return', 'Portfolio_Risk'],\n",
    "    'Value': [\n",
    "        portfolio_results['stock_weight'],\n",
    "        1 - portfolio_results['stock_weight'],\n",
    "        portfolio_results['combined_returns'],\n",
    "        portfolio_results['combined_risk']\n",
    "    ]\n",
    "})\n",
    "portfolio_summary.to_csv('results/portfolio_summary.csv', index=False)\n",
    "\n",
    "print(\"\\nResults saved to 'results' directory:\")\n",
    "print(\"1. stock_portfolio_allocation.csv\")\n",
    "print(\"2. mf_portfolio_allocation.csv\")\n",
    "print(\"3. portfolio_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.1 Results Verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify results directory contents\n",
    "print(\"\\nVerifying results directory contents:\")\n",
    "!ls -l results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 14: Mount Google Drive and save everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and save everything\n",
    "from google.colab import drive\n",
    "from datetime import datetime\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create timestamped folder\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "folder_name = f'Investment_Recommendation_{timestamp}'\n",
    "!mkdir -p /content/drive/MyDrive/{folder_name}\n",
    "\n",
    "# Save notebook\n",
    "!cp /content/Investment_Recommendation_System.ipynb /content/drive/MyDrive/{folder_name}/\n",
    "\n",
    "# Save results\n",
    "!cp -r results /content/drive/MyDrive/{folder_name}/\n",
    "\n",
    "# Save models (if you saved them)\n",
    "!cp -r models /content/drive/MyDrive/{folder_name}/\n",
    "\n",
    "print(f\"\\nAll files saved to Google Drive folder: {folder_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
